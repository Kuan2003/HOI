{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract gaze features in VidHOI dataset\n",
    "Use head detection and gaze following methods to extract feature maps for all key frames in VidHOI dataset. Check face is inside a person bbox. Store them in a separate buffer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuan/miniconda3/envs/hoi/lib/python3.9/site-packages/torch/torch_version.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../modules/object_tracking/yolov5\")\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import numpy as np\n",
    "import shelve\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from modules.object_tracking import HeadDetection\n",
    "from modules.gaze_following import GazeFollowing\n",
    "from modules.gaze_following.head_association import assign_human_head_video\n",
    "from common.vidhoi_dataset import VidHOIDataset\n",
    "from common.transforms import YOLOv5Transform\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Head Detection YOLOv5 Module...\n",
      "{'DEEPSORT': {'MODEL_TYPE': 'osnet_x1_0', 'MAX_DIST': 0.2, 'MIN_CONFIDENCE': 0.3, 'NMS_MAX_OVERLAP': 0.5, 'MAX_IOU_DISTANCE': 0.7, 'MAX_AGE': 70, 'MAX_TIME_SINCE_UPDATE': 3, 'N_INIT': 3, 'NN_BUDGET': 100}, 'YOLOv5': {'CONF_THRESHOLD': 0.4, 'IOU_THRESHOLD': 0.5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model summary: 308 layers, 21041679 parameters, 0 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stride: 32\n",
      "2 available objects: {0: 'person', 1: 'head'}\n",
      "Head Detection Module Initialization Finished.\n",
      "Initializing Gaze Following \"Detecting Attended Visual Targets in Video\" Module...\n",
      "Gaze Following Module Initialization Finished.\n"
     ]
    }
   ],
   "source": [
    "# Load Head Tracking and Gaze Following modules\n",
    "head_detection_module = HeadDetection(\n",
    "    crowd_human_weight_path=\"../weights/yolov5/crowdhuman_yolov5m.pt\",\n",
    "    config_path=\"../configs/object_tracking.yaml\",\n",
    "    device=device,\n",
    ")\n",
    "gaze_following_module = GazeFollowing(\n",
    "    weight_path=\"../weights/detecting_attended/model_videoatttarget.pt\",\n",
    "    config_path=\"../configs/gaze_following.yaml\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "img_size = 640\n",
    "yolov5_stride = head_detection_module.yolov5_stride\n",
    "# NOTE adjust this tolerance and method\n",
    "head_matching_iou_thres = 0.7\n",
    "head_matching_method = \"hungarian\"\n",
    "# head_matching_method = \"greedy\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Dataset - Skip if not available\n",
    "# vidhoi_val_dataset = VidHOIDataset(\n",
    "#     annotations_file=\"/home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/VidHOI_annotation/val_frame_annots.json\",\n",
    "#     frames_dir=\"/home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/images\",\n",
    "#     transform=YOLOv5Transform(img_size, yolov5_stride),\n",
    "#     min_length=1,\n",
    "#     max_length=999999,\n",
    "#     max_human_num=999999,\n",
    "#     annotation_mode=\"clip\",\n",
    "# )\n",
    "# vidhoi_val_dataloader = DataLoader(vidhoi_val_dataset, batch_size=None, shuffle=False)\n",
    "# output_val_head_filename = \"/home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/VidHOI_gaze/val_frame_heads_gt_bbox\"\n",
    "# output_val_gaze_filename = \"/home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/VidHOI_gaze/val_frame_gazes_gt_bbox\"\n",
    "# output_val_inout_filename = \"/home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/VidHOI_gaze/val_frame_inout_gt_bbox\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_val_head_dict = shelve.open(output_val_head_filename)\n",
    "output_val_gaze_dict = shelve.open(output_val_gaze_filename)\n",
    "output_val_inout_dict = shelve.open(output_val_inout_filename)\n",
    "# For each video, first detect heads\n",
    "t = tqdm(vidhoi_val_dataloader)\n",
    "for frames, annotations, meta_info in t:\n",
    "    video_name = meta_info['video_name']\n",
    "    t.set_description(f\"{video_name}\")\n",
    "    t.refresh()\n",
    "    original_frames = meta_info[\"original_frames\"]\n",
    "    bboxes = annotations[\"bboxes\"]\n",
    "    ids = annotations[\"ids\"]\n",
    "    labels = annotations[\"labels\"]\n",
    "    bboxes = np.array(bboxes)\n",
    "    ids = np.array(ids)\n",
    "    labels = np.array(labels)\n",
    "    with torch.no_grad():\n",
    "        video_head_bbox_list = assign_human_head_video(\n",
    "            frames,\n",
    "            original_frames,\n",
    "            bboxes,\n",
    "            ids,\n",
    "            labels,\n",
    "            head_detection_module,\n",
    "            head_matching_iou_thres,\n",
    "            device,\n",
    "            method=head_matching_method,\n",
    "        )\n",
    "    # assign video head bbox list to its name\n",
    "    output_val_head_dict[video_name] = video_head_bbox_list\n",
    "    output_val_head_dict.sync()\n",
    "\n",
    "    # for each head bbox, detect gaze\n",
    "    video_gaze_list = []\n",
    "    video_inout_list = []\n",
    "    hx_memory = {}\n",
    "    for i, (head_bboxes, frame0) in enumerate(\n",
    "        zip(video_head_bbox_list, original_frames)\n",
    "    ):\n",
    "        t.set_description(f\"{video_name}/{meta_info['frame_ids'][i]}, {i}/{len(video_head_bbox_list) - 1}: \")\n",
    "        t.refresh()\n",
    "        frame_gaze_dict = {}\n",
    "        frame_inout_dict = {}\n",
    "        for human_id, head_bbox in head_bboxes.items():\n",
    "            t.set_postfix_str(f\"{head_bbox}\")\n",
    "            # no head found for this human_id\n",
    "            if len(head_bbox) == 0:\n",
    "                frame_gaze_dict[human_id] = []\n",
    "                frame_inout_dict[human_id] = []\n",
    "                continue\n",
    "            # check hidden state memory\n",
    "            if human_id in hx_memory:\n",
    "                hidden_state = hx_memory[human_id]\n",
    "            else:\n",
    "                hidden_state = None\n",
    "            with torch.no_grad():\n",
    "                (heatmap, inout, hx, _, _, _,) = gaze_following_module.detect_one(\n",
    "                    frame0.numpy(),\n",
    "                    head_bbox,\n",
    "                    hidden_state,\n",
    "                    draw=False,\n",
    "                )\n",
    "            hx_memory[human_id] = (hx[0].detach(), hx[1].detach())\n",
    "            # process heatmap 64x64 (not include inout), store inout info separately\n",
    "            # softmax inout, value = probability of gaze inside the scene\n",
    "            inout_modulated = 1 / (1 + np.exp(-inout))\n",
    "            # assign heatmap and in_out to human_id\n",
    "            frame_gaze_dict[human_id] = heatmap\n",
    "            frame_inout_dict[human_id] = inout_modulated\n",
    "        # append frame heatmap and inout dict to video heatmap list\n",
    "        video_gaze_list.append(frame_gaze_dict)\n",
    "        video_inout_list.append(frame_inout_dict)\n",
    "    # assign video heatmap list to its name\n",
    "    output_val_gaze_dict[video_name] = video_gaze_list\n",
    "    output_val_gaze_dict.sync()\n",
    "    output_val_inout_dict[video_name] = video_inout_list\n",
    "    output_val_inout_dict.sync()\n",
    "\n",
    "output_val_head_dict.close()\n",
    "output_val_gaze_dict.close()\n",
    "output_val_inout_dict.close()\n",
    "print(f\"Head bboxes dumped to {output_val_head_filename}\")\n",
    "print(f\"Gaze heatmaps dumped to {output_val_gaze_filename}\")\n",
    "print(f\"Gaze inout dumped to {output_val_inout_filename}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "vidhoi_train_dataset = VidHOIDataset(\n",
    "    annotations_file=\"/home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/VidHOI_annotation/train_frame_annots.json\",\n",
    "    frames_dir=\"/home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/images\",\n",
    "    transform=YOLOv5Transform(img_size, yolov5_stride),\n",
    "    min_length=1,\n",
    "    max_length=999999,\n",
    "    max_human_num=999999,\n",
    "    annotation_mode=\"clip\",\n",
    ")\n",
    "vidhoi_train_dataloader = DataLoader(\n",
    "    vidhoi_train_dataset, batch_size=None, shuffle=False\n",
    ")\n",
    "output_train_head_filename = \"/home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/VidHOI_gaze/train_frame_heads_gt_bbox\"\n",
    "output_train_gaze_filename = \"/home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/VidHOI_gaze/train_frame_gazes_gt_bbox\"\n",
    "output_train_inout_filename = \"/home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/VidHOI_gaze/train_frame_inout_gt_bbox\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fabac5c1-p01_cup_stable_video007_p01/stable/000149, 108/108: : 100%|██████████| 48/48 [05:51<00:00,  7.33s/it, [        460          98         581         267]]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head bboxes dumped to /home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/VidHOI_gaze/train_frame_heads_gt_bbox\n",
      "Gaze heatmaps dumped to /home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/VidHOI_gaze/train_frame_gazes_gt_bbox\n",
      "Gaze inout dumped to /home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/VidHOI_gaze/train_frame_inout_gt_bbox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_train_head_dict = shelve.open(output_train_head_filename)\n",
    "output_train_gaze_dict = shelve.open(output_train_gaze_filename)\n",
    "output_train_inout_dict = shelve.open(output_train_inout_filename)\n",
    "# For each video, first detect heads\n",
    "t = tqdm(vidhoi_train_dataloader)\n",
    "for frames, annotations, meta_info in t:\n",
    "    video_name = meta_info['video_name']\n",
    "    t.set_description(f\"{video_name}\")\n",
    "    t.refresh()\n",
    "    original_frames = meta_info[\"original_frames\"]\n",
    "    bboxes = annotations[\"bboxes\"]\n",
    "    ids = annotations[\"ids\"]\n",
    "    labels = annotations[\"labels\"]\n",
    "    bboxes = np.array(bboxes)\n",
    "    ids = np.array(ids)\n",
    "    labels = np.array(labels)\n",
    "    with torch.no_grad():\n",
    "        video_head_bbox_list = assign_human_head_video(\n",
    "            frames,\n",
    "            original_frames,\n",
    "            bboxes,\n",
    "            ids,\n",
    "            labels,\n",
    "            head_detection_module,\n",
    "            head_matching_iou_thres,\n",
    "            device,\n",
    "            method=head_matching_method,\n",
    "        )\n",
    "    # assign video head bbox list to its name\n",
    "    output_train_head_dict[video_name] = video_head_bbox_list\n",
    "    output_train_head_dict.sync()\n",
    "\n",
    "    # for each head bbox, detect gaze\n",
    "    video_gaze_list = []\n",
    "    video_inout_list = []\n",
    "    hx_memory = {}\n",
    "    for i, (head_bboxes, frame0) in enumerate(\n",
    "        zip(video_head_bbox_list, original_frames)\n",
    "    ):\n",
    "        t.set_description(f\"{video_name}/{meta_info['frame_ids'][i]}, {i}/{len(video_head_bbox_list) - 1}: \")\n",
    "        t.refresh()\n",
    "        frame_gaze_dict = {}\n",
    "        frame_inout_dict = {}\n",
    "        for human_id, head_bbox in head_bboxes.items():\n",
    "            t.set_postfix_str(f\"{head_bbox}\")\n",
    "            # no head found for this human_id\n",
    "            if len(head_bbox) == 0:\n",
    "                frame_gaze_dict[human_id] = []\n",
    "                frame_inout_dict[human_id] = []\n",
    "                continue\n",
    "            # check hidden state memory\n",
    "            if human_id in hx_memory:\n",
    "                hidden_state = hx_memory[human_id]\n",
    "            else:\n",
    "                hidden_state = None\n",
    "            with torch.no_grad():\n",
    "                (heatmap, inout, hx, _, _, _,) = gaze_following_module.detect_one(\n",
    "                    frame0.numpy(),\n",
    "                    head_bbox,\n",
    "                    hidden_state,\n",
    "                    draw=False,\n",
    "                )\n",
    "            hx_memory[human_id] = (hx[0].detach(), hx[1].detach())\n",
    "            # process heatmap 64x64 (not include inout), store inout info separately\n",
    "            # softmax inout, value = probability of gaze inside the scene\n",
    "            inout_modulated = 1 / (1 + np.exp(-inout))\n",
    "            # assign heatmap and in_out to human_id\n",
    "            frame_gaze_dict[human_id] = heatmap\n",
    "            frame_inout_dict[human_id] = inout_modulated\n",
    "        # append frame heatmap and inout dict to video heatmap list\n",
    "        video_gaze_list.append(frame_gaze_dict)\n",
    "        video_inout_list.append(frame_inout_dict)\n",
    "    # assign video heatmap list to its name\n",
    "    output_train_gaze_dict[video_name] = video_gaze_list\n",
    "    output_train_gaze_dict.sync()\n",
    "    output_train_inout_dict[video_name] = video_inout_list\n",
    "    output_train_inout_dict.sync()\n",
    "\n",
    "output_train_head_dict.close()\n",
    "output_train_gaze_dict.close()\n",
    "output_train_inout_dict.close()\n",
    "print(f\"Head bboxes dumped to {output_train_head_filename}\")\n",
    "print(f\"Gaze heatmaps dumped to {output_train_gaze_filename}\")\n",
    "print(f\"Gaze inout dumped to {output_train_inout_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
