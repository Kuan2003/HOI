{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc0f57d",
   "metadata": {},
   "source": [
    "# Convert VidHOI Custom Dataset to YOLOv5 Format\n",
    "\n",
    "This notebook converts your custom VidHOI dataset annotations to YOLOv5 training format.\n",
    "\n",
    "**Your dataset has 4 object classes:**\n",
    "- person (class 0)\n",
    "- cup (class 1)\n",
    "- plate (class 2)\n",
    "- box (class 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945e45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241f90e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object classes: ['person', 'cup', 'plate', 'box']\n",
      "Class mapping: {'person': 0, 'cup': 1, 'plate': 2, 'box': 3}\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "dataset_path = Path(\"/home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi\")\n",
    "output_root = Path(\"/home/kuan/Work_Space/Thuc_tap/HOI/yolov5_dataset\")\n",
    "\n",
    "# Your 4 object classes\n",
    "object_classes = ['person', 'cup', 'plate', 'box']\n",
    "name_to_idx = {name: idx for idx, name in enumerate(object_classes)}\n",
    "\n",
    "print(f\"Object classes: {object_classes}\")\n",
    "print(f\"Class mapping: {name_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb692c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotations: 5329\n",
      "\n",
      "First annotation example:\n",
      "{\n",
      "  \"video_folder\": \"0484310a-p01_abort_after_touch_video002_p01_abort\",\n",
      "  \"video_id\": \"after\",\n",
      "  \"frame_id\": \"000004\",\n",
      "  \"video_fps\": 10,\n",
      "  \"height\": 720,\n",
      "  \"width\": 1280,\n",
      "  \"middle_frame_timestamp\": 0.4,\n",
      "  \"person_box\": {\n",
      "    \"xmin\": 269,\n",
      "    \"ymin\": 65,\n",
      "    \"xmax\": 595,\n",
      "    \"ymax\": 719\n",
      "  },\n",
      "  \"object_box\": {\n",
      "    \"xmin\": 645,\n",
      "    \"ymin\": 437,\n",
      "    \"xmax\": 725,\n",
      "    \"ymax\": 527\n",
      "  },\n",
      "  \"person_id\": 1,\n",
      "  \"object_id\": 2,\n",
      "  \"object_class\": 1,\n",
      "  \"action_class\": 2\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load annotation file\n",
    "train_annotation_path = dataset_path / \"VidHOI_annotation\" / \"train_frame_annots.json\"\n",
    "\n",
    "with train_annotation_path.open(\"r\") as f:\n",
    "    train_annotations = json.load(f)\n",
    "\n",
    "print(f\"Total annotations: {len(train_annotations)}\")\n",
    "print(f\"\\nFirst annotation example:\")\n",
    "print(json.dumps(train_annotations[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9309b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object counts:\n",
      "  person: 0\n",
      "  cup: 2434\n",
      "  plate: 1244\n",
      "  box: 1651\n",
      "\n",
      "Total unique videos: 48\n",
      "Total unique frames: 5099\n"
     ]
    }
   ],
   "source": [
    "# Analyze dataset\n",
    "object_count = defaultdict(int)\n",
    "video_frames = defaultdict(set)\n",
    "\n",
    "for anno in train_annotations:\n",
    "    # Count objects (object_class: 0=person, 1=cup, 2=plate, 3=box)\n",
    "    obj_class_idx = anno['object_class']\n",
    "    if obj_class_idx < len(object_classes):\n",
    "        object_count[object_classes[obj_class_idx]] += 1\n",
    "    \n",
    "    # Track unique frames per video\n",
    "    video_key = f\"{anno['video_folder']}_{anno['video_id']}\"\n",
    "    video_frames[video_key].add(anno['frame_id'])\n",
    "\n",
    "print(\"Object counts:\")\n",
    "for obj_name in object_classes:\n",
    "    print(f\"  {obj_name}: {object_count[obj_name]}\")\n",
    "\n",
    "print(f\"\\nTotal unique videos: {len(video_frames)}\")\n",
    "total_frames = sum(len(frames) for frames in video_frames.values())\n",
    "print(f\"Total unique frames: {total_frames}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c16b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bbox_to_yolo(bbox, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert bbox from [xmin, ymin, xmax, ymax] to YOLO format [x_center, y_center, width, height]\n",
    "    All values normalized to [0, 1]\n",
    "    \"\"\"\n",
    "    x_center = ((bbox['xmax'] + bbox['xmin']) / 2) / img_width\n",
    "    y_center = ((bbox['ymax'] + bbox['ymin']) / 2) / img_height\n",
    "    width = (bbox['xmax'] - bbox['xmin']) / img_width\n",
    "    height = (bbox['ymax'] - bbox['ymin']) / img_height\n",
    "    \n",
    "    return x_center, y_center, width, height\n",
    "\n",
    "def generate_yolov5_annotations(annotations, output_path, images_dir):\n",
    "    \"\"\"\n",
    "    Generate YOLOv5 format annotations from VidHOI annotations\n",
    "    \"\"\"\n",
    "    # Group annotations by frame\n",
    "    frame_annotations = defaultdict(list)\n",
    "    \n",
    "    for anno in annotations:\n",
    "        video_folder = anno['video_folder']\n",
    "        video_id = anno['video_id']\n",
    "        frame_id = anno['frame_id']\n",
    "        \n",
    "        # Create unique frame key\n",
    "        frame_key = f\"{video_folder}/{video_id}/{frame_id}\"\n",
    "        frame_annotations[frame_key].append(anno)\n",
    "    \n",
    "    # Create output directories\n",
    "    labels_dir = output_path / \"labels\"\n",
    "    labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    image_list = []\n",
    "    missing_images = 0\n",
    "    \n",
    "    for frame_key, annos in tqdm(frame_annotations.items(), desc=\"Converting annotations\"):\n",
    "        parts = frame_key.split('/')\n",
    "        video_folder = parts[0]\n",
    "        video_id = parts[1]\n",
    "        frame_id = parts[2]\n",
    "        \n",
    "        # Create label subdirectories\n",
    "        label_subdir = labels_dir / video_folder / video_id\n",
    "        label_subdir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Label file path\n",
    "        label_file = label_subdir / f\"{frame_id}.txt\"\n",
    "        \n",
    "        # Get image dimensions from first annotation\n",
    "        img_height = annos[0]['height']\n",
    "        img_width = annos[0]['width']\n",
    "        \n",
    "        # Write YOLO format annotations\n",
    "        yolo_lines = []\n",
    "        for anno in annos:\n",
    "            # Person bbox\n",
    "            person_class = 0  # person is always class 0\n",
    "            person_bbox = anno['person_box']\n",
    "            x_c, y_c, w, h = convert_bbox_to_yolo(person_bbox, img_width, img_height)\n",
    "            yolo_lines.append(f\"{person_class} {x_c:.6f} {y_c:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "            \n",
    "            # Object bbox\n",
    "            object_class = anno['object_class']\n",
    "            object_bbox = anno['object_box']\n",
    "            x_c, y_c, w, h = convert_bbox_to_yolo(object_bbox, img_width, img_height)\n",
    "            yolo_lines.append(f\"{object_class} {x_c:.6f} {y_c:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "        \n",
    "        # Remove duplicates (same person/object might appear multiple times)\n",
    "        yolo_lines = list(set(yolo_lines))\n",
    "        \n",
    "        # Write label file\n",
    "        with label_file.open('w') as f:\n",
    "            f.writelines(yolo_lines)\n",
    "        \n",
    "        # Image path - FIXED: use video_id prefix in filename\n",
    "        image_path = images_dir / video_folder / video_id / f\"{video_id}_{frame_id}.jpg\"\n",
    "        if image_path.exists():\n",
    "            image_list.append(str(image_path.absolute()) + \"\\n\")\n",
    "        else:\n",
    "            missing_images += 1\n",
    "    \n",
    "    if missing_images > 0:\n",
    "        print(f\"Warning: {missing_images} images not found\")\n",
    "    \n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "386fde6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating YOLOv5 annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting annotations: 100%|██████████| 5099/5099 [00:00<00:00, 7849.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 5099 training images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate YOLOv5 format annotations\n",
    "images_dir = dataset_path / \"images\"\n",
    "\n",
    "print(\"Generating YOLOv5 annotations...\")\n",
    "image_list = generate_yolov5_annotations(train_annotations, output_root, images_dir)\n",
    "\n",
    "print(f\"\\nGenerated {len(image_list)} training images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c8d355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking image directory structure...\n",
      "✓ Images directory exists: /home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/images\n",
      "\n",
      "First 3 subdirectories in images/:\n",
      "  - 9da22909-p01_hold_random_video005_p01\n",
      "    - random\n",
      "      Files: ['random_000031.jpg', 'random_000009.jpg', 'random_000020.jpg']\n",
      "  - 81dd6fb1-p01_box_stable_video003_p01\n",
      "    - stable\n",
      "      Files: ['stable_000005.jpg', 'stable_000128.jpg', 'stable_000105.jpg']\n",
      "  - defa59b1-p01_abort_early_video001_p01\n",
      "    - early\n",
      "      Files: ['early_000108.jpg', 'early_000074.jpg', 'early_000058.jpg']\n",
      "\n",
      "Sample annotation:\n",
      "  video_folder: 0484310a-p01_abort_after_touch_video002_p01_abort\n",
      "  video_id: after\n",
      "  frame_id: 000004\n",
      "\n",
      "Testing different file naming patterns:\n",
      "  000004.jpg                     -> ✗ Not found\n",
      "  after_000004.jpg               -> ✓ EXISTS\n",
      "    Full path: /home/kuan/Work_Space/Thuc_tap/HOI/dataset_vidhoi/images/0484310a-p01_abort_after_touch_video002_p01_abort/after/after_000004.jpg\n",
      "\n",
      "Actual files in after:\n",
      "  - after_000067.jpg\n",
      "  - after_000065.jpg\n",
      "  - after_000039.jpg\n",
      "  - after_000098.jpg\n",
      "  - after_000097.jpg\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check actual image directory structure\n",
    "print(\"Checking image directory structure...\")\n",
    "images_dir = dataset_path / \"images\"\n",
    "\n",
    "# Check if images directory exists\n",
    "if not images_dir.exists():\n",
    "    print(f\"ERROR: Images directory does not exist: {images_dir}\")\n",
    "else:\n",
    "    print(f\"✓ Images directory exists: {images_dir}\")\n",
    "    \n",
    "    # List first few subdirectories\n",
    "    subdirs = list(images_dir.iterdir())[:3]\n",
    "    print(f\"\\nFirst 3 subdirectories in images/:\")\n",
    "    for subdir in subdirs:\n",
    "        print(f\"  - {subdir.name}\")\n",
    "        if subdir.is_dir():\n",
    "            video_dirs = list(subdir.iterdir())[:2]\n",
    "            for vdir in video_dirs:\n",
    "                print(f\"    - {vdir.name}\")\n",
    "                if vdir.is_dir():\n",
    "                    image_files = list(vdir.glob(\"*\"))[:3]\n",
    "                    print(f\"      Files: {[f.name for f in image_files]}\")\n",
    "\n",
    "# Check expected path vs actual path\n",
    "sample_anno = train_annotations[0]\n",
    "print(f\"\\nSample annotation:\")\n",
    "print(f\"  video_folder: {sample_anno['video_folder']}\")\n",
    "print(f\"  video_id: {sample_anno['video_id']}\")\n",
    "print(f\"  frame_id: {sample_anno['frame_id']}\")\n",
    "\n",
    "# Test different naming patterns\n",
    "print(f\"\\nTesting different file naming patterns:\")\n",
    "test_patterns = [\n",
    "    f\"{sample_anno['frame_id']}.jpg\",  # 000004.jpg\n",
    "    f\"{sample_anno['video_id']}_{sample_anno['frame_id']}.jpg\",  # after_000004.jpg\n",
    "    f\"{sample_anno['frame_id'][1:]}.jpg\",  # 00004.jpg (remove leading zero)\n",
    "    f\"{int(sample_anno['frame_id'])}.jpg\",  # 4.jpg (as integer)\n",
    "]\n",
    "\n",
    "for pattern in test_patterns:\n",
    "    test_path = images_dir / sample_anno['video_folder'] / sample_anno['video_id'] / pattern\n",
    "    exists = test_path.exists()\n",
    "    print(f\"  {pattern:30s} -> {'✓ EXISTS' if exists else '✗ Not found'}\")\n",
    "    if exists:\n",
    "        print(f\"    Full path: {test_path}\")\n",
    "        break\n",
    "\n",
    "# If none found, list actual files in that directory\n",
    "video_dir = images_dir / sample_anno['video_folder'] / sample_anno['video_id']\n",
    "if video_dir.exists():\n",
    "    actual_files = list(video_dir.glob(\"*.jpg\"))[:5]\n",
    "    print(f\"\\nActual files in {video_dir.name}:\")\n",
    "    for f in actual_files:\n",
    "        print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c855dc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 4079\n",
      "Validation images: 1020\n",
      "\n",
      "Saved train list to: /home/kuan/Work_Space/Thuc_tap/HOI/yolov5_dataset/train.txt\n",
      "Saved val list to: /home/kuan/Work_Space/Thuc_tap/HOI/yolov5_dataset/val.txt\n"
     ]
    }
   ],
   "source": [
    "# Split into train and validation sets (80/20 split)\n",
    "from random import shuffle, seed\n",
    "\n",
    "seed(42)  # For reproducibility\n",
    "shuffle(image_list)\n",
    "\n",
    "split_idx = int(len(image_list) * 0.8)\n",
    "train_list = image_list[:split_idx]\n",
    "val_list = image_list[split_idx:]\n",
    "\n",
    "print(f\"Training images: {len(train_list)}\")\n",
    "print(f\"Validation images: {len(val_list)}\")\n",
    "\n",
    "# Save image lists\n",
    "train_txt = output_root / \"train.txt\"\n",
    "val_txt = output_root / \"val.txt\"\n",
    "\n",
    "with train_txt.open('w') as f:\n",
    "    f.writelines(train_list)\n",
    "\n",
    "with val_txt.open('w') as f:\n",
    "    f.writelines(val_list)\n",
    "\n",
    "print(f\"\\nSaved train list to: {train_txt}\")\n",
    "print(f\"Saved val list to: {val_txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94c5c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created YOLOv5 data config: /home/kuan/Work_Space/Thuc_tap/HOI/yolov5_dataset/vidhoi_custom.yaml\n",
      "\n",
      "Config content:\n",
      "# VidHOI Custom Dataset Configuration\n",
      "# Path to dataset root\n",
      "path: /home/kuan/Work_Space/Thuc_tap/HOI/yolov5_dataset\n",
      "\n",
      "# Train/val image lists\n",
      "train: train.txt\n",
      "val: val.txt\n",
      "\n",
      "# Number of classes\n",
      "nc: 4\n",
      "\n",
      "# Class names\n",
      "names: ['person', 'cup', 'plate', 'box']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create YOLOv5 data configuration file\n",
    "data_yaml = output_root / \"vidhoi_custom.yaml\"\n",
    "\n",
    "yaml_content = f\"\"\"# VidHOI Custom Dataset Configuration\n",
    "# Path to dataset root\n",
    "path: {output_root.absolute()}\n",
    "\n",
    "# Train/val image lists\n",
    "train: train.txt\n",
    "val: val.txt\n",
    "\n",
    "# Number of classes\n",
    "nc: {len(object_classes)}\n",
    "\n",
    "# Class names\n",
    "names: {object_classes}\n",
    "\"\"\"\n",
    "\n",
    "with data_yaml.open('w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(f\"Created YOLOv5 data config: {data_yaml}\")\n",
    "print(\"\\nConfig content:\")\n",
    "print(yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d4e3088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample label file: labels/0484310a-p01_abort_after_touch_video002_p01_abort/after/000093.txt\n",
      "\n",
      "Content:\n",
      "1 0.535156 0.669444 0.062500 0.125000\n",
      "0 0.387109 0.544444 0.367969 0.908333\n",
      "\n",
      "\n",
      "Format: class x_center y_center width height\n",
      "Classes: ['person', 'cup', 'plate', 'box']\n"
     ]
    }
   ],
   "source": [
    "# Verify generated annotations\n",
    "import random\n",
    "\n",
    "# Pick a random label file to inspect\n",
    "label_files = list((output_root / \"labels\").rglob(\"*.txt\"))\n",
    "sample_label = random.choice(label_files)\n",
    "\n",
    "print(f\"Sample label file: {sample_label.relative_to(output_root)}\")\n",
    "print(\"\\nContent:\")\n",
    "with sample_label.open('r') as f:\n",
    "    content = f.read()\n",
    "    print(content)\n",
    "\n",
    "print(\"\\nFormat: class x_center y_center width height\")\n",
    "print(f\"Classes: {object_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c9c3a",
   "metadata": {},
   "source": [
    "## Training YOLOv5\n",
    "\n",
    "After running this notebook, you can train YOLOv5 with:\n",
    "\n",
    "```bash\n",
    "cd modules/object_tracking/yolov5\n",
    "\n",
    "# Train from scratch (recommended for completely new objects)\n",
    "python train.py --img 640 --batch 16 --epochs 100 \\\n",
    "    --data /home/kuan/Work_Space/Thuc_tap/HOI/yolov5_dataset/vidhoi_custom.yaml \\\n",
    "    --weights '' --cfg yolov5s.yaml \\\n",
    "    --name vidhoi_custom --cache\n",
    "\n",
    "# Or fine-tune from COCO pretrained weights (faster convergence)\n",
    "python train.py --img 640 --batch 16 --epochs 100 \\\n",
    "    --data /home/kuan/Work_Space/Thuc_tap/HOI/yolov5_dataset/vidhoi_custom.yaml \\\n",
    "    --weights yolov5s.pt \\\n",
    "    --name vidhoi_custom_finetune --cache\n",
    "```\n",
    "\n",
    "**Note:** Even though cup, plate, box are not in COCO, the pretrained weights can still help because:\n",
    "1. The feature extraction layers learned general visual patterns\n",
    "2. Only the final detection head needs to be retrained for your 4 classes\n",
    "3. This usually converges faster than training from scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
